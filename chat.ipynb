{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QDYcisZkTc8",
        "outputId": "2f9d9218-b5c1-4f15-90d7-deb5a6d20f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 13 21:26:47 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P0    28W /  70W |   3747MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TFDDKkJfkQjy"
      },
      "outputs": [],
      "source": [
        "! pip install -q langchain transformers sentence_transformers llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "aGCX8IBekbNd"
      },
      "outputs": [],
      "source": [
        "from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTListIndex, PromptHelper\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import LLMPredictor, ServiceContext\n",
        "from llama_index import GPTVectorStoreIndex\n",
        "import torch\n",
        "from langchain.llms.base import LLM\n",
        "from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nVcWX65LkfG6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class customLLM(LLM):\n",
        "    model_name = \"google/flan-t5-large\"\n",
        "    pipeline = pipeline(\"text2text-generation\", model=model_name, device=0, model_kwargs={\"torch_dtype\":torch.bfloat16})\n",
        "\n",
        "    def _call(self, prompt, stop=None):\n",
        "        return self.pipeline(prompt, max_length=9999)[0][\"generated_text\"]\n",
        "\n",
        "    def _identifying_params(self):\n",
        "        return {\"name_of_model\": self.model_name}\n",
        "\n",
        "    def _llm_type(self):\n",
        "        return \"custom\"\n",
        "\n",
        "\n",
        "llm_predictor = LLMPredictor(llm=customLLM())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Noq5r9_jkiVT"
      },
      "outputs": [],
      "source": [
        "hfemb = HuggingFaceEmbeddings()\n",
        "embed_model = LangchainEmbedding(hfemb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qAord7Hdp0Ps"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.getLogger().setLevel(logging.CRITICAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Nww8v92EKex6"
      },
      "outputs": [],
      "source": [
        "# temp fix for running shell commands on Google Colab\n",
        "\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "42iQRUlYhbgn"
      },
      "outputs": [],
      "source": [
        "!pip install gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "p92tHG-fhHS_"
      },
      "outputs": [],
      "source": [
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Nya0xFWjxyQq"
      },
      "outputs": [],
      "source": [
        "index = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gTjRf-Cok8Ew"
      },
      "outputs": [],
      "source": [
        "from llama_index import Document\n",
        "def build_the_bot(input_text):\n",
        "  text_list = [input_text]\n",
        "  documents = []\n",
        "  for t in text_list:\n",
        "    doc = Document()\n",
        "    #doc.content = t\n",
        "    documents.append(doc)\n",
        "  #documents = [Document(t) for t in text_list]\n",
        "  global index\n",
        "  service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model)\n",
        "  index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "  return('Index saved successfully!!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Vz1Dufj9jyMh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "  def chat(chat_history, user_input):\n",
        "    query_engine = index.as_query_engine()\n",
        "    bot_response = query_engine.query(user_input)\n",
        "\n",
        "    if bot_response and bot_response.response:\n",
        "        response = \"\"\n",
        "\n",
        "        if isinstance(bot_response.response, list):\n",
        "            for item in bot_response.response:\n",
        "                if isinstance(item, str):\n",
        "                    response += item\n",
        "        elif isinstance(bot_response.response, str):\n",
        "            response = bot_response.response\n",
        "\n",
        "        return chat_history + [(user_input, response)]\n",
        "    else:\n",
        "        return chat_history\n",
        "    #response = bot_response.response)\n",
        "    #response = \"\"\n",
        "\n",
        "    #for letter in bot_response.response:\n",
        "     # response += letter + \"\"\n",
        "\n",
        "      #return chat_history + [(user_input, response)]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "oCL2RqVZp3AM",
        "outputId": "47664669-b3cc-49da-8b04-ee9796a5f181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://1fe19ebb16211096a9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1fe19ebb16211096a9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown('# Q&A Bot with Hugging Face Models')\n",
        "    with gr.Tab(\"Input Text Document\"):\n",
        "        text_input = gr.Textbox()\n",
        "        text_output = gr.Textbox()\n",
        "        text_button = gr.Button(\"Build the Bot!!!\")\n",
        "        text_button.click(build_the_bot, text_input, text_output)\n",
        "    with gr.Tab(\"Knowledge Bot\"):\n",
        "#          inputbox = gr.Textbox(\"Input your text to build a Q&A Bot here.....\")\n",
        "          chatbot = gr.Chatbot()\n",
        "          message = gr.Textbox (\"What is this document about?\")\n",
        "          message.submit(chat, [chatbot, message], chatbot)\n",
        "\n",
        "demo.queue().launch(debug = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwFjGtsrhZKf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}